# Implicit Depth Learning in Convolutional Neural Networks On 3D Datasets
### Group Members: Yegor Kuznetsov, Zander Brumbaugh, Colton Jobes

## Project Summary
Our project aims to test our hypothesis that depth information can be reliably predicted from the latent representation from an autoencoder, thus proving depth information is learned by the autoencoder when training. We additionally compare the accuracy between several models in order to provide further evidence in support of our hypothesis. Furthermore, we propose a two-parameter, generalized framework to determine whether a feature is implicitly learned given a feature and computer vision task.

## Datasets
We used the Diode dataset (Vasiljevic et al., 2019) and NYU Depth V2 dataset (Silberman et al., 2012). The NYU data was provided in a .mat format which we then preprocessed into a useable format with Python and later a Dataset object for use with PyTorch. We used this set for development as it was considerably smaller than the Diode dataset but consequently had lower image and depth map resolution. We then trained our final model on the images and depth maps from the Diode dataset.

## Methodology
We began by implementing our autoencoder based on the publicly available VQ-VAE (Vector Quantized - Variational Autoencoder) used by DALL-E from OpenAI. We removed the vector quantization, downsized the images to 256 x 256 pixels, and trained the model until convergence. For all training across the project, we used a Nvidia 3080Ti GPU on one of our own devices. The autoencoder took six hours to train and produced good results. With the autoencoder trained, we were ready to create the network for the linear probe. The probe is a 1 x k matrix, where we used k = 10, that is trained to predict depth information in a given image given its latent representation from the autoencoder. For the training of the linear probe, we used L2 loss and trained until convergence. The probe took only ten minutes to train and we qualitatively found the predicted depth maps to be largely represntative of the ground truth maps.

Having shown that depth information is learned by the autoencoder, we wanted to compare the linear probe's accuracy with the accuracy of a convolutional neural network training explicitly to predict depth information to determine the importance of depth information in the representation from the autoencoder. That is, if a network trained to predict depth information achieves only marginally better accuracy, that further supports our hypothesis and implies depth information is a critical feature in the task of the autoencoder.
We created a CNN with four groups, with each group having two residual blocks, and each block having four convolutional layers. We additionally include a 1x1 convolution layer for prediction. This is the same architecture as the encoder portion of the autoencoder with the additional convolutional layer for prediction. We use this similar architecture, the same dataset used to train the probe, and the same downsampling techniques to create fair conditions for the comparison of the accuracies between both models. We used L2 loss and trained the CNN until convergence. The CNN achieved a lower loss over the test we created from 25,000 random downsampled images not included in the training data.

## Results
We found our hypothesis was to some extent true. The accuracy that the linear probe achieved shows that depth information is learnable from the latent representation from the autoencoder, supporting our hypothesis that depth information is implicitly learned by the autoencoder during training.
The CNN trained to predict depth information stably achieved achieved a loss 25% lower than that of the trained linear probe. This matches our expectation but we qualitatively assess that the predicted depth maps are similar. Exactly how this information is extracted appears to be mostly from color, with lighter colors correlating with further distances. We believe this could be related to images with the sky or other similar examples even when augmentation was applied.

## Discussion
We believe that we would have been able to produce even stronger evidence that depth information is implicitly learned by CNNs on 3D datasets if we used a task different than an autoencoder.
Because our hypothesis was true, we propose a generalized framework for proving that a specified feature is learned in the training of a given computer vision task.  This framework requires only a dataset with the ground truth data for the feature you wish to know is implicitly learned and the encoded input from the model for the task.
